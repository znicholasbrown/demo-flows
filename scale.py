from prefect import flow, task
from prefect_dask import DaskTaskRunner
from prefect.assets import Asset, AssetProperties
from prefect.assets import materialize
from prefect.artifacts import create_table_artifact, create_markdown_artifact, create_progress_artifact
from prefect.events import emit_event
import time
from typing import List, Dict, Any

# Define a simple base asset for the demo
base_asset = Asset(
    key="sftp://scaling/base",
    properties=AssetProperties(
        name="Base directory",
        description="Base directory for scaling test",
        owners=["nicholas"],
        url="https://prefect.io",
    )
)

@materialize(base_asset)
def materialize_base_asset(metadata: dict | None = None):
    """Materialize the base asset"""

    if metadata:
        base_asset.add_metadata(metadata)

    return {"status": "materialized", "timestamp": time.time()}

@task
def create_task_asset(task_id: int) -> Asset:
    """Create a dynamic asset for each task"""
    new_asset = Asset(
        key=f"sftp://scaling/task-{task_id}-data",
        properties=AssetProperties(
            name=f"Task {task_id} file",
            description=f"File created by task {task_id}",
            owners=["nicholas"],
            url="https://prefect.io",
        )
    )
    
    @materialize(new_asset, asset_deps=[base_asset], by="python")
    def materialize_task_asset(task_num: int):
        return {"task_number": task_num, "status": "completed"}
    
    materialize_task_asset(task_id)
    return new_asset

@task
def process_data(task_id: int, batch_size: int = 100) -> Dict[str, Any]:
    """Process data for a given task ID"""
    # Simulate some processing work
    time.sleep(0.1)  # Small delay to simulate work
    
    # Create deterministic data based on task_id
    processed_data = {
        "task_id": task_id,
        "records_processed": batch_size * (task_id + 1),
        "processing_time": 0.1,
        "status": "success"
    }
    
    # Emit an event for this task
    emit_event(
        event=f"task.{task_id}.completed",
        resource={"prefect.resource.id": f"demo.task.{task_id}"},
        payload={"records_processed": processed_data["records_processed"]}
    )
    
    return processed_data

@task
def create_task_artifact(task_id: int, data: Dict[str, Any]) -> None:
    """Create artifacts for each task"""
    # Create a table artifact with task results
    table_data = {
        "columns": ["Task ID", "Records Processed", "Processing Time", "Status"],
        "data": [
            [data["task_id"], data["records_processed"], data["processing_time"], data["status"]]
        ]
    }
    
    create_table_artifact(
        table=table_data,
        key=f"task-{task_id}-results",
        description=f"Results for task {task_id}"
    )
    
    # Create a markdown artifact with summary
    markdown_content = f"""
# Task {task_id} Summary

## Processing Results
- **Task ID**: {task_id}
- **Records Processed**: {data['records_processed']:,}
- **Processing Time**: {data['processing_time']:.2f}s
- **Status**: {data['status']}

## Performance Metrics
- **Throughput**: {data['records_processed'] / data['processing_time']:.0f} records/second
- **Efficiency**: {(task_id + 1) * 100}%

---
*Generated by scaling test*
    """
    
    create_markdown_artifact(
        markdown=markdown_content,
        key=f"task-{task_id}-summary",
        description=f"Summary report for task {task_id}"
    )

@task
def create_progress_artifacts(completed_tasks: int, total_tasks: int) -> None:
    """Create progress artifacts showing completion status"""
    progress_percentage = (completed_tasks / total_tasks) * 100
    
    create_progress_artifact(
        progress=progress_percentage,
        key="overall-progress",
        description=f"Overall progress: {completed_tasks}/{total_tasks} tasks completed"
    )

@task
def aggregate_results(all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Aggregate results from all tasks"""
    total_records = sum(result["records_processed"] for result in all_results)
    total_time = sum(result["processing_time"] for result in all_results)
    avg_records_per_task = total_records / len(all_results)
    
    aggregated_data = {
        "total_tasks": len(all_results),
        "total_records_processed": total_records,
        "total_processing_time": total_time,
        "average_records_per_task": avg_records_per_task,
        "throughput": total_records / total_time if total_time > 0 else 0
    }
    
    # Create a summary table artifact
    summary_table = {
        "columns": ["Metric", "Value"],
        "data": [
            ["Total Tasks", aggregated_data["total_tasks"]],
            ["Total Records", f"{aggregated_data['total_records_processed']:,}"],
            ["Total Time", f"{aggregated_data['total_processing_time']:.2f}s"],
            ["Avg Records/Task", f"{aggregated_data['average_records_per_task']:.0f}"],
            ["Throughput", f"{aggregated_data['throughput']:.0f} records/s"]
        ]
    }
    
    create_table_artifact(
        table=summary_table,
        key="aggregated-results",
        description="Aggregated results from all parallel tasks"
    )
    
    # Emit completion event
    emit_event(
        event="flow.completed",
        resource={"prefect.resource.id": "demo.scale-flow"},
        payload=aggregated_data
    )
    
    return aggregated_data

@flow(task_runner=DaskTaskRunner())
def scale_flow(n: int = 5, batch_size: int = 100):
    """
    A scalable demo flow that demonstrates parallel execution with assets, artifacts, and events.
    
    Args:
        n: Number of parallel tasks to execute
        batch_size: Base batch size for processing (will be multiplied by task_id + 1)
    """
    # Materialize the base asset
    materialize_base_asset()
    
    # Create assets for each task
    task_assets = []
    for i in range(n):
        asset = create_task_asset(i)
        task_assets.append(asset)
    
    # Process data in parallel
    results = []
    for i in range(n):
        result = process_data.submit(i, batch_size)
        results.append(result)
    
    # Wait for all processing to complete
    processed_results = [result.result() for result in results]
    
    # Create artifacts for each task
    for i, result in enumerate(processed_results):
        create_task_artifact.submit(i, result)
    
    # Create progress artifacts
    create_progress_artifacts(len(processed_results), n)
    
    # Aggregate and summarize results
    final_results = aggregate_results(processed_results)
    
    materialize_base_asset({
        "tasks_executed": n,
        "total_records_processed": final_results["total_records_processed"],
        "execution_timestamp": time.time()
    })

    return {
        "task_assets": task_assets,
        "results": processed_results,
        "summary": final_results
    }

if __name__ == "__main__":
    # Example usage with default parameters
    scale_flow()
