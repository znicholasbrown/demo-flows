"""
Factory pattern script to generate 400 Prefect flows with various name patterns.
"""
import os
# Set Prefect logging level before importing prefect
os.environ.setdefault('PREFECT_LOGGING_LEVEL', 'WARNING')

import asyncio
import time
import argparse
import sys
import io
import contextlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from prefect import flow


# Configuration: prefix patterns and their counts (1200 total flows)
FLOW_PATTERNS = {
    # Original patterns (400 flows)
    "bird": 50,
    "latte": 50,
    "keyboard": 25,
    "cloud": 50,
    "ocean": 50,
    "mountain": 25,
    "river": 25,
    "forest": 50,
    "desert": 25,
    "city": 50,
    # Animal/wildlife category (+140 flows)
    "cat": 50,
    "dog": 50,
    "fish": 40,
    # Beverage/food category (+140 flows)
    "coffee": 50,
    "pizza": 50,
    "sushi": 40,
    # Technology/device category (+90 flows)
    "laptop": 50,
    "phone": 40,
    # Nature/weather category (+90 flows)
    "rain": 50,
    "wind": 40,
    # Nature/water category (+90 flows)
    "lake": 50,
    "stream": 40,
    # Nature/place/terrain category (+130 flows)
    "valley": 50,
    "canyon": 40,
    "hill": 40,
    # Place/urban category (+120 flows)
    "town": 50,
    "district": 50,
    "avenue": 20,
}

# Tags for each prefix pattern (using existing tags only)
FLOW_TAGS = {
    # Original
    "bird": ["animal", "wildlife"],
    "latte": ["beverage", "food"],
    "keyboard": ["technology", "device"],
    "cloud": ["nature", "weather"],
    "ocean": ["nature", "water"],
    "mountain": ["nature", "place", "terrain"],
    "river": ["nature", "water"],
    "forest": ["nature", "place", "terrain"],
    "desert": ["nature", "place", "terrain"],
    "city": ["place", "urban"],
    # New - animal/wildlife
    "cat": ["animal", "wildlife"],
    "dog": ["animal", "wildlife"],
    "fish": ["animal", "wildlife"],
    # New - beverage/food
    "coffee": ["beverage", "food"],
    "pizza": ["beverage", "food"],
    "sushi": ["beverage", "food"],
    # New - technology/device
    "laptop": ["technology", "device"],
    "phone": ["technology", "device"],
    # New - nature/weather
    "rain": ["nature", "weather"],
    "wind": ["nature", "weather"],
    # New - nature/water
    "lake": ["nature", "water"],
    "stream": ["nature", "water"],
    # New - nature/place/terrain
    "valley": ["nature", "place", "terrain"],
    "canyon": ["nature", "place", "terrain"],
    "hill": ["nature", "place", "terrain"],
    # New - place/urban
    "town": ["place", "urban"],
    "district": ["place", "urban"],
    "avenue": ["place", "urban"],
}


# Generate base flows dynamically - one flow per prefix (with deployments)
# We use exec to create actual functions at module level that Prefect can find
flows = {}
for prefix in FLOW_PATTERNS.keys():
    # Create the flow function code as a string
    flow_code = f'''
@flow(name="{prefix}")
def {prefix}():
    """No-op flow generated by factory pattern for {prefix}."""
    pass
'''
    # Execute the code to create the function in the current namespace
    exec(flow_code, globals())

    # Store reference in flows dict
    flows[prefix] = globals()[prefix]


# Generate additional individual flows (no deployments, just for running)
# These are individual flow instances with numbered names
ADDITIONAL_FLOW_PATTERNS = {
    # Animal/wildlife themed (190 flows)
    "hawk": 50,
    "eagle": 50,
    "lion": 50,
    "tiger": 40,
    # Beverage/food themed (190 flows)
    "tea": 50,
    "burger": 50,
    "ramen": 50,
    "taco": 40,
    # Technology/device themed (115 flows)
    "tablet": 50,
    "monitor": 40,
    "mouse": 25,
    # Nature/weather themed (140 flows)
    "snow": 50,
    "fog": 50,
    "storm": 40,
    # Nature/water themed (165 flows)
    "pond": 50,
    "creek": 50,
    "bay": 40,
    "waterfall": 25,
    # Nature/place/terrain themed (230 flows)
    "cliff": 50,
    "mesa": 50,
    "dune": 50,
    "peak": 40,
    "ridge": 40,
    # Place/urban themed (170 flows)
    "street": 50,
    "plaza": 50,
    "boulevard": 50,
    "lane": 20,
}

# Tags for additional flows (using existing tag combinations)
ADDITIONAL_FLOW_TAGS = {
    # Animal/wildlife
    "hawk": ["animal", "wildlife"],
    "eagle": ["animal", "wildlife"],
    "lion": ["animal", "wildlife"],
    "tiger": ["animal", "wildlife"],
    # Beverage/food
    "tea": ["beverage", "food"],
    "burger": ["beverage", "food"],
    "ramen": ["beverage", "food"],
    "taco": ["beverage", "food"],
    # Technology/device
    "tablet": ["technology", "device"],
    "monitor": ["technology", "device"],
    "mouse": ["technology", "device"],
    # Nature/weather
    "snow": ["nature", "weather"],
    "fog": ["nature", "weather"],
    "storm": ["nature", "weather"],
    # Nature/water
    "pond": ["nature", "water"],
    "creek": ["nature", "water"],
    "bay": ["nature", "water"],
    "waterfall": ["nature", "water"],
    # Nature/place/terrain
    "cliff": ["nature", "place", "terrain"],
    "mesa": ["nature", "place", "terrain"],
    "dune": ["nature", "place", "terrain"],
    "peak": ["nature", "place", "terrain"],
    "ridge": ["nature", "place", "terrain"],
    # Place/urban
    "street": ["place", "urban"],
    "plaza": ["place", "urban"],
    "boulevard": ["place", "urban"],
    "lane": ["place", "urban"],
}

# Generate additional individual flows (these won't have deployments)
additional_flows = []
for prefix, count in ADDITIONAL_FLOW_PATTERNS.items():
    for i in range(1, count + 1):
        flow_name = f"{prefix}-{i:03d}"
        # Create individual flow
        flow_code = f'''
@flow(name="{flow_name}")
def {flow_name.replace("-", "_")}():
    """No-op additional flow generated by factory pattern."""
    pass
'''
        exec(flow_code, globals())
        additional_flows.append(globals()[flow_name.replace("-", "_")])


class ProgressTracker:
    """Track progress of threaded operations with real-time updates."""

    def __init__(self, total: int, operation: str = "operations"):
        self.total = total
        self.completed = 0
        self.operation = operation
        self.start_time = time.time()
        self.errors = []

    def increment(self, error=None):
        """Increment the counter and print progress."""
        self.completed += 1
        if error:
            self.errors.append(error)

        percent = (self.completed / self.total) * 100
        elapsed = time.time() - self.start_time
        rate = self.completed / elapsed if elapsed > 0 else 0

        # Print progress bar
        bar_length = 40
        filled = int(bar_length * self.completed / self.total)
        bar = '█' * filled + '░' * (bar_length - filled)

        error_str = f" | {len(self.errors)} errors" if self.errors else ""
        sys.stdout.write(f'\r[{bar}] {self.completed}/{self.total} {self.operation} ({percent:.1f}%) | {rate:.1f}/s{error_str}')
        sys.stdout.flush()

        if self.completed == self.total:
            print()  # New line at completion

    def print_errors(self):
        """Print all errors that occurred."""
        if self.errors:
            print(f"\n❌ Errors occurred during {self.operation}:")
            for i, error in enumerate(self.errors, 1):
                print(f"\n{i}. {error}")


def run_single_flow(flow_func, run_number: int):
    """Run a single flow instance."""
    try:
        flow_func()
        return (True, None)
    except Exception as e:
        error_msg = f"Flow run {run_number} ({flow_func.name}): {str(e)}"
        return (False, error_msg)


def run_all_flows(max_workers: int = 50):
    """Run all flows concurrently using threads (multiple runs per flow + additional flows)."""
    # Calculate total number of runs
    base_runs = sum(FLOW_PATTERNS.values())
    additional_runs = len(additional_flows)
    total_runs = base_runs + additional_runs

    print(f"Running {len(flows)} base flows ({base_runs} runs) + {len(additional_flows)} additional flows ({additional_runs} runs)")
    print(f"Total: {total_runs} flow runs with {max_workers} workers...")
    print("Flow output suppressed - errors only will be shown.\n")

    progress = ProgressTracker(total_runs, "flow runs")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all flow runs
        futures = []
        run_number = 1

        # Submit base flows (multiple runs each)
        for prefix, count in FLOW_PATTERNS.items():
            flow_func = flows[prefix]
            for _ in range(count):
                future = executor.submit(run_single_flow, flow_func, run_number)
                futures.append(future)
                run_number += 1

        # Submit additional flows (one run each)
        for flow_func in additional_flows:
            future = executor.submit(run_single_flow, flow_func, run_number)
            futures.append(future)
            run_number += 1

        # Process completions
        for future in as_completed(futures):
            success, error = future.result()
            progress.increment(error=error)

    elapsed = time.time() - start_time
    successful = total_runs - len(progress.errors)

    print(f"\n✓ Successfully completed {successful}/{total_runs} flow runs")
    print(f"  Base flows: {base_runs} runs")
    print(f"  Additional flows: {additional_runs} runs")
    print(f"Total time: {elapsed:.2f}s")

    progress.print_errors()


def deploy_flow_batch(prefix: str, count: int, work_pool: str, progress: ProgressTracker):
    """
    Deploy all deployments for a single flow.
    This function runs in a single thread and deploys all deployments for one flow sequentially.

    Args:
        prefix: The flow prefix/name
        count: Number of deployments to create for this flow
        work_pool: Work pool name
        progress: Progress tracker instance

    Returns:
        Tuple of (successful_count, errors_list)
    """
    flow_func = flows[prefix]
    tags = FLOW_TAGS.get(prefix, [])
    errors = []

    for i in range(1, count + 1):
        deployment_name = f"{prefix}-{i:03d}"

        try:
            # Deploy from git source
            deployment_args = {
                "name": deployment_name,
                "tags": tags,
            }
            if work_pool:
                deployment_args["work_pool_name"] = work_pool

            flow.from_source(
                source="https://github.com/znicholasbrown/demo-flows.git",
                entrypoint=f"flow_factory.py:{prefix}"
            ).deploy(**deployment_args)

            progress.increment(error=None)
        except Exception as e:
            error_msg = f"Deployment {deployment_name} (flow: {prefix}): {str(e)}"
            errors.append(error_msg)
            progress.increment(error=error_msg)

    return (count - len(errors), errors)


def deploy_all_flows(work_pool: str = None, max_workers: int = 4):
    """
    Create deployments for all flows using git source.
    Uses 4 worker threads, with each thread handling all deployments for specific flows.
    """
    # Calculate total number of deployments
    total_deployments = sum(FLOW_PATTERNS.values())

    print(f"Creating {total_deployments} deployments for {len(flows)} flows from git source...\n")
    print("Git source: https://github.com/znicholasbrown/demo-flows.git")
    print(f"Entrypoint file: flow_factory.py")
    if work_pool:
        print(f"Work pool: {work_pool}")
    print()

    print("Tags by prefix:")
    for prefix, tags in FLOW_TAGS.items():
        print(f"  - {prefix}: {', '.join(tags)}")
    print()

    print(f"Deploying with {max_workers} worker threads (each thread handles all deployments for specific flows)...")
    print("Deployment output suppressed - errors only will be shown.\n")

    progress = ProgressTracker(total_deployments, "deployments")
    start_time = time.time()

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit one job per flow (each job deploys all deployments for that flow)
        futures = []
        for prefix, count in FLOW_PATTERNS.items():
            future = executor.submit(deploy_flow_batch, prefix, count, work_pool, progress)
            futures.append((future, prefix, count))

        # Wait for all flows to complete their deployments
        for future, prefix, count in futures:
            try:
                successful, errors = future.result()
            except Exception as e:
                print(f"\nError processing flow {prefix}: {e}")

    elapsed = time.time() - start_time
    successful = total_deployments - len(progress.errors)

    print(f"\n✓ Successfully deployed {successful}/{total_deployments} deployments across {len(flows)} flows")
    print(f"Total time: {elapsed:.2f}s")

    progress.print_errors()


if __name__ == "__main__":
    total_deployments = sum(FLOW_PATTERNS.values())
    total_flows = len(flows) + len(additional_flows)
    total_runs = sum(FLOW_PATTERNS.values()) + len(additional_flows)

    parser = argparse.ArgumentParser(
        description=f"Generate and run/deploy {total_flows} Prefect flows ({len(flows)} with deployments, {len(additional_flows)} standalone)"
    )
    parser.add_argument(
        "--deploy",
        action="store_true",
        help="Create deployments for base flows (additional flows have no deployments)"
    )
    parser.add_argument(
        "--work-pool",
        type=str,
        default=None,
        help="Work pool name for deployments (optional)"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=None,
        help="Maximum number of concurrent workers (default: 50 for runs, 4 for deployments)"
    )
    args = parser.parse_args()

    print(f"Generated {total_flows} total flows:")
    print(f"  - {len(flows)} base flows with {total_deployments} deployments")
    print(f"  - {len(additional_flows)} additional flows (no deployments)")
    print()

    if args.deploy:
        print("Deployment patterns (base flows only):")
        for prefix, count in FLOW_PATTERNS.items():
            print(f"  - {prefix}: {count} deployments")
        print()
        max_workers = args.max_workers if args.max_workers else 4
        deploy_all_flows(work_pool=args.work_pool, max_workers=max_workers)
    else:
        max_workers = args.max_workers if args.max_workers else 50
        run_all_flows(max_workers=max_workers)
